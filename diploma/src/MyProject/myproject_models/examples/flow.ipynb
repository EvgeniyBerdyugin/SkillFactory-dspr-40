{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeypointRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (keypoint_head): KeypointRCNNHeads(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "    (keypoint_predictor): KeypointRCNNPredictor(\n",
       "      (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Используем модель предложенную в задании, так как она хорошо работает\n",
    "model = keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_and_limbs_for_one_person(img: np.ndarray, keypoints: list) -> np.ndarray:\n",
    "    \"\"\"Function drawing keypoints and limbs to target image\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): target image\n",
    "        keypoints (list): keypoints\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: output image\n",
    "    \"\"\"\n",
    "    # создаём копию изображений\n",
    "    img_copy = img.copy()\n",
    "    point_color = (0, 255, 0)\n",
    "    limb_color = (0, 0, 255)\n",
    "    limbs = [[2, 0], [2, 4], [1, 0], [1, 3], [6, 8], [8, 10], [5, 7], [7, 9], [12, 14], \n",
    "             [14, 16], [11, 13], [13, 15], [6, 5], [12, 11], [6, 12], [5, 11]]\n",
    "    for keypoint in keypoints:\n",
    "        # рисуем кружок радиуса 5 вокруг точки\n",
    "        cv2.circle(img_copy, tuple(keypoint), 5, point_color, -1)\n",
    "    for limb in limbs:\n",
    "        point0 = tuple(keypoints[limb[0]])\n",
    "        point1 = tuple(keypoints[limb[1]])\n",
    "        cv2.line(img_copy, point0, point1, limb_color, 2)\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции добавляют и убирают 1 к массиву для расчета аффинного преобразования\n",
    "pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "unpad = lambda x: x[:, :-1]\n",
    "\n",
    "\n",
    "def affine_transform(points1: np.ndarray, points2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Affine transform points2 to points1\n",
    "\n",
    "    Args:\n",
    "        points1 (np.ndarray): ethalon points\n",
    "        points2 (np.ndarray): target points\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: output points\n",
    "    \"\"\"\n",
    "    Y = pad(points1)\n",
    "    X = pad(points2)\n",
    "    A, _, _, _ = np.linalg.lstsq(X, Y)\n",
    "    A[np.abs(A) < 1e-10] = 0\n",
    "    transform = lambda x: unpad(np.dot(pad(x), A))\n",
    "    points2_1 = transform(points2)\n",
    "    return points2_1\n",
    "\n",
    "\n",
    "def get_similarity(points1: np.ndarray, points2: np.ndarray) -> float:\n",
    "    \"\"\"Get cosine similarity of two poses\n",
    "\n",
    "    Args:\n",
    "        points1 (np.ndarray): point of first pose\n",
    "        points2 (np.ndarray): points of second pose\n",
    "\n",
    "    Returns:\n",
    "        float: cosine similarity\n",
    "    \"\"\"\n",
    "    points2 = affine_transform(points1, points2)\n",
    "    sim = torch.nn.functional.cosine_similarity(torch.Tensor(points1), torch.Tensor(points2))\n",
    "    return sim.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_distance(pose1: np.ndarray, pose2: np.ndarray, conf1: float) -> float:\n",
    "    \"\"\"Get weighted distance of two poses\n",
    "\n",
    "    Args:\n",
    "        pose1 (np.ndarray): point of first pose\n",
    "        pose2 (np.ndarray): points of second pose\n",
    "        conf1 (float): confidence of predicted points of first pose\n",
    "\n",
    "    Returns:\n",
    "        float: weighted distance\n",
    "    \"\"\"\n",
    "    pose2 = affine_transform(pose1, pose2)\n",
    "    sum1 = 1 / np.sum(conf1)\n",
    "    sum2 = 0\n",
    "    for i in range(len(pose1)):\n",
    "        sum2 += conf1[i] * abs(math.hypot(pose1[i][0] - pose2[i][0], pose1[i][1] - pose2[i][1]))\n",
    "    weighted_dist = sum1 * sum2\n",
    "\n",
    "    return weighted_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_images(im0: np.ndarray, im1: np.ndarray, thres: float=0.8) -> [list, float, float]:\n",
    "    \"\"\"compare poses on two images\n",
    "\n",
    "    Args:\n",
    "        im0 (np.ndarray): first image\n",
    "        im1 (np.ndarray): second image\n",
    "        thres (float, optional): threshold for keypoints confidence. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        [list, float, float]: retutns prediction from keypoint detection model, cosine symilarity\n",
    "        and weighted distance between predicted poses on both images\n",
    "    \"\"\"\n",
    "    trans = transforms.Compose([transforms.ToTensor()])\n",
    "    tensor_im0 = trans(im0.copy()).cuda()\n",
    "    tensor_im1 = trans(im1.copy()).cuda()\n",
    "    model.eval()\n",
    "    res = model([tensor_im0, tensor_im1])\n",
    "    score0 = res[0]['scores'][0]\n",
    "    score1 = res[1]['scores'][0]\n",
    "    if score0 >= thres and score1 >= thres:\n",
    "        points0 = res[0]['keypoints'][0].cpu().detach().numpy()[:, :-1]\n",
    "        points1 = res[1]['keypoints'][0].cpu().detach().numpy()[:, :-1]\n",
    "        sim = get_similarity(points0, points1)\n",
    "        conf0 = res[0]['keypoints_scores'][0].to('cpu').detach().numpy()\n",
    "        wd = weight_distance(points0, points1, conf0)\n",
    "    else:\n",
    "        sim = 0, \n",
    "        wd = 100\n",
    "    return res, sim, wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_videos(model: any, source0: str, source1: str, sim_low_thres: float=0.99, \n",
    "                     sim_high_thres: float=0.999, wd_low_thres: float=10, \n",
    "                     wd_high_thres: float=30, show_video: bool=False) -> None:\n",
    "    \"\"\"compare two videos by each frame a save videofile with results\n",
    "\n",
    "    Args:\n",
    "        model (any): keypoints prediction vodel\n",
    "        source0 (str): source of first video\n",
    "        source1 (str): source of second video\n",
    "        sim_low_thres (float, optional): similarity lower threshold accept not \n",
    "        similar poses. Defaults to 0.99.\n",
    "        sim_high_thres (float, optional): lower threshold accept highly similar \n",
    "        poses. Defaults to 0.999.\n",
    "        wd_low_thres (float, optional): weighted distance lower threshold accept \n",
    "        highly similar poses. Defaults to 10.\n",
    "        wd_high_thres (float, optional): weighted distance lower threshold accept \n",
    "        not similar poses. Defaults to 30.\n",
    "        show_video (bool, optional): show video wihle processing flag. Defaults to False.\n",
    "    \"\"\"\n",
    "    name0 = source0.split('/')[-1].split('.')[0]\n",
    "    name1 = source1.split('/')[-1].split('.')[0]\n",
    "    vcap0 = cv2.VideoCapture(source0)\n",
    "    size = (int(vcap0.get(3)), int(vcap0.get(4)))\n",
    "    total_size = (size[0] * 2, size[1])\n",
    "    vcap1 = cv2.VideoCapture(source1)\n",
    "    out = cv2.VideoWriter(f'{name0}-{name1}.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 24, total_size)\n",
    "    ok_flag = True\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    sim_total = 0\n",
    "    wd_total = 0\n",
    "    while ok_flag:\n",
    "        ret0, frame0 = vcap0.read()\n",
    "        ret1, frame1 = vcap1.read()\n",
    "        if not ret0 or not ret1:\n",
    "            print(\"Frame is empty\")\n",
    "            break\n",
    "        else:\n",
    "            frame1 = cv2.resize(frame1, size)\n",
    "            res, sim, wd = compare_two_images(frame0, frame1)\n",
    "            sim_text = f\"similarity: {round(sim, 6)}\"\n",
    "            wd_text =  f\"WD: {round(wd, 2)}\"\n",
    "            points0 = res[0]['keypoints'].detach().to('cpu')[0][:, :2].int().tolist()\n",
    "            points1 = res[1]['keypoints'].detach().to('cpu')[0][:, :2].int().tolist()\n",
    "            frame0 = draw_keypoints_and_limbs_for_one_person(frame0, points0)\n",
    "            frame1 = draw_keypoints_and_limbs_for_one_person(frame1, points1)\n",
    "            if sim > sim_high_thres:\n",
    "                sim_color = (0, 255, 0)\n",
    "            elif sim > sim_low_thres and sim <= sim_high_thres:\n",
    "                sim_color = (0, 255, 255)\n",
    "            else:\n",
    "                sim_color = (0, 0, 255)\n",
    "            if wd < wd_low_thres:\n",
    "                wd_color = (0, 255, 0)\n",
    "            elif wd < wd_high_thres and wd >= wd_low_thres:\n",
    "                wd_color = (0, 255, 255)\n",
    "            else:\n",
    "                wd_color = (0, 0, 255)\n",
    "            frame0 = cv2.putText(frame0, 'Teacher', (250, 930), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            frame1 = cv2.putText(frame1, sim_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, sim_color, 2)\n",
    "            frame1 = cv2.putText(frame1, wd_text, (200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, wd_color, 2)\n",
    "            frame1 = cv2.putText(frame1, 'Studient', (250, 930), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            frame = np.concatenate([frame0, frame1], axis=1)\n",
    "            out.write(frame)\n",
    "            if show_video:\n",
    "                cv2.imshow('VIDEO', frame)\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                ok_flag = False\n",
    "            sim_total += sim\n",
    "            wd_total += wd\n",
    "        i += 1\n",
    "    sim_total /= i\n",
    "    wd_total /= i\n",
    "    last_frame = np.zeros((total_size[1], total_size[0], 3)).astype('uint8')\n",
    "    text = f'Your result is: similarity - {round(sim_total, 6)}, WD - {round(wd_total, 2)}'\n",
    "    last_frame = cv2.putText(last_frame, text, (100, 480), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 4)\n",
    "    for i in range(30):\n",
    "        out.write(last_frame)\n",
    "    out.release()\n",
    "    if show_video:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame is empty\n"
     ]
    }
   ],
   "source": [
    "source0 = '../../myproject_data/2.mp4'\n",
    "source1 = '../../myproject_data/6.mp4'\n",
    "compare_two_videos(model, source0, source1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame is empty\n"
     ]
    }
   ],
   "source": [
    "source0 = '../../myproject_data/2.mp4'\n",
    "source1 = '../../myproject_data/4.mp4'\n",
    "compare_two_videos(model, source0, source1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
